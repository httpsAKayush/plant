{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50b4d6af-2b38-44ba-bbcb-c9cd7fe10b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 16:04:14.300863: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-31 16:04:14.385150: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756636454.407316    5840 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756636454.414725    5840 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756636454.428002    5840 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756636454.428026    5840 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756636454.428027    5840 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756636454.428028    5840 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-31 16:04:14.434150: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import unsloth\n",
    "import transformers\n",
    "import trl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fa3a66-88c1-4281-a298-332d958f84a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.8.10: Fast Gemma3N patching. Transformers: 4.56.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3050 Laptop GPU. Num GPUs = 1. Max memory: 3.68 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Gemma3N does not support SDPA - switching to fast eager.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be376535293749548d1a8673c706af1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/3.72G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import unsloth\n",
    "from unsloth import FastModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "fourbit_models = [\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3n-E4B-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3n-E4B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3n-E2B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
    "] \n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3n-E4B-it\",\n",
    "    dtype = None, \n",
    "    max_seq_length = 1024, \n",
    "    load_in_4bit = True,  \n",
    "    full_finetuning = False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d721e4b6-3f90-4583-b62f-38bd24e719db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install unsloth\n",
    "\n",
    "# !pip install --no-deps --upgrade transformers\n",
    "# !pip install --no-deps --upgrade timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207b1672-c4e1-4aa2-9001-54c0cc05d937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install unsloth_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ab2429-4996-4ae2-9e74-697202e94205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "import gc\n",
    "\n",
    "def do_gemma_3n_inference(model, tokenizer, messages, max_new_tokens = 128):\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt = True, \n",
    "        tokenize = True,\n",
    "        return_dict = True,\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=1.0,\n",
    "        top_p=0.95,\n",
    "        top_k=64,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    generated_text = tokenizer.decode(\n",
    "        output_ids[0][inputs['input_ids'].shape[-1]:], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    # Cleanup to reduce VRAM usage\n",
    "    del inputs\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a865b915-9cc5-45b0-9f2a-1f95b3a51f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "torch._dynamo.config.cache_size_limit = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c03ffa-05d1-40ac-9ca6-09f4f7479034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unified_training_data(df1, df2):\n",
    "    \"\"\"Combine both datasets into a unified training format\"\"\"\n",
    "    \n",
    "    print(\"ðŸ”„ Creating unified training dataset...\")\n",
    "    \n",
    "    unified_data = []\n",
    "    \n",
    "    # Process Dataset 1 (Detailed)\n",
    "    print(\"Processing detailed dataset...\")\n",
    "    for _, row in df1.iterrows():\n",
    "        # Basic recommendation\n",
    "        basic_prompt = (\n",
    "            f\"N: {row['Nitrogen']}, P: {row['Phosphorus']}, K: {row['Potassium']}, \"\n",
    "            f\"temperature: {row['Temperature']:.2f}, humidity: {row['Humidity']:.2f}, \"\n",
    "            f\"pH: {row['pH_Value']:.2f}, rainfall: {row['Rainfall']:.2f}\"\n",
    "        )\n",
    "        basic_response = f\"Recommended crop: {row['Crop']}\"\n",
    "        \n",
    "        # Detailed recommendation with soil type and variety\n",
    "        detailed_prompt = (\n",
    "            f\"N: {row['Nitrogen']}, P: {row['Phosphorus']}, K: {row['Potassium']}, \"\n",
    "            f\"temperature: {row['Temperature']:.2f}, humidity: {row['Humidity']:.2f}, \"\n",
    "            f\"pH: {row['pH_Value']:.2f}, rainfall: {row['Rainfall']:.2f}, \"\n",
    "            f\"soil_type: {row['Soil_Type']}\"\n",
    "        )\n",
    "        detailed_response = (\n",
    "            f\"Recommended crop: {row['Crop']}. \"\n",
    "            f\"Suitable soil type: {row['Soil_Type']}. \"\n",
    "            f\"Recommended variety: {row['Variety']}. \"\n",
    "            f\"This combination is optimal for the given soil and climate conditions.\"\n",
    "        )\n",
    "        \n",
    "        # Add both variations\n",
    "        unified_data.append({\"prompt\": basic_prompt, \"response\": basic_response, \"source\": \"dataset1_basic\"})\n",
    "        unified_data.append({\"prompt\": detailed_prompt, \"response\": detailed_response, \"source\": \"dataset1_detailed\"})\n",
    "    \n",
    "    # Process Dataset 2 (Simple)\n",
    "    print(\"Processing simple dataset...\")\n",
    "    for _, row in df2.iterrows():\n",
    "        prompt = (\n",
    "            f\"N: {row['N']}, P: {row['P']}, K: {row['K']}, \"\n",
    "            f\"temperature: {row['temperature']:.2f}, humidity: {row['humidity']:.2f}, \"\n",
    "            f\"pH: {row['ph']:.2f}, rainfall: {row['rainfall']:.2f}\"\n",
    "        )\n",
    "        response = f\"Recommended crop: {row['label']}\"\n",
    "        \n",
    "        unified_data.append({\"prompt\": prompt, \"response\": response, \"source\": \"dataset2\"})\n",
    "    \n",
    "    print(f\"âœ… Created {len(unified_data)} training examples\")\n",
    "    return unified_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31dc451-84ee-43ed-93ae-2d23240a6d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2 = pd.read_csv('Crop_recommendation.csv')\n",
    "df1  = pd.read_csv('sensor_Crop_Dataset.csv')\n",
    "unified_data = create_unified_training_data(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da221e9b-d5c4-4d1b-a07e-96590a6a0c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
