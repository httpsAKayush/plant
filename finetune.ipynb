{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50b4d6af-2b38-44ba-bbcb-c9cd7fe10b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 22:33:42.474342: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-31 22:33:42.485594: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756659822.499761    9344 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756659822.504318    9344 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756659822.515447    9344 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756659822.515468    9344 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756659822.515470    9344 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756659822.515471    9344 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-31 22:33:42.519159: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import unsloth\n",
    "import transformers\n",
    "import trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1fa3a66-88c1-4281-a298-332d958f84a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.8.10: Fast Gemma3 patching. Transformers: 4.56.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3050 Laptop GPU. Num GPUs = 1. Max memory: 3.68 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import unsloth\n",
    "from unsloth import FastModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "fourbit_models = [\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3n-E4B-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3n-E4B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3n-E2B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
    "]   \n",
    "\n",
    "# model, tokenizer = FastModel.from_pretrained(\n",
    "#     model_name = \"unsloth/gemma-3n-E2B-it\",\n",
    "#     dtype = None, \n",
    "#     max_seq_length = 1024, \n",
    "#     load_in_4bit = True,  \n",
    "#     full_finetuning = False\n",
    "# )\n",
    "\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name=\"unsloth/gemma-3-1b-it\",\n",
    "    dtype=torch.float16,  # Use a lower precision for computation\n",
    "    max_seq_length=1024,\n",
    "    load_in_4bit=True,\n",
    "    full_finetuning=False,\n",
    "    # llm_int8_enable_fp32_cpu_offload=True,\n",
    "    # device_map=\"auto\"  # Automatically assigns layers to available devices\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28be0c2b-1b8a-4f27-ad7a-58bfcefb7332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# from unsloth import FastModel\n",
    "# import torch\n",
    "\n",
    "# # Load the model and tokenizer manually\n",
    "# model_name = \"unsloth/gemma-3n-E2B-it\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     device_map=\"auto\",  # Automatically assign layers to available devices\n",
    "#     torch_dtype=torch.float16,  # Use float16 for reduced memory usage\n",
    "#     load_in_4bit=True,  # Load the model in 4-bit precision\n",
    "#     trust_remote_code=True  # Trust the remote code for model loading\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# # Convert the model into a FastModel forz optimized inference\n",
    "# fast_model = FastModel.from_model(model)\n",
    "\n",
    "# # Now you can use fast_model and tokenizer for inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d721e4b6-3f90-4583-b62f-38bd24e719db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install unsloth\n",
    "\n",
    "# !pip install --no-deps --upgrade transformers\n",
    "# !pip install --no-deps --upgrade timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "207b1672-c4e1-4aa2-9001-54c0cc05d937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install unsloth_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfbf546a-5aa5-4345-a6b1-98914d3ec49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install peft\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# # Reduce VRAM usage\n",
    "# torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8ab2429-4996-4ae2-9e74-697202e94205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "import gc\n",
    "\n",
    "def do_gemma_3n_inference(model, tokenizer, messages, max_new_tokens = 128):\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt = True, \n",
    "        tokenize = True,\n",
    "        return_dict = True,\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=1.0,\n",
    "        top_p=0.95,\n",
    "        top_k=64,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    generated_text = tokenizer.decode(\n",
    "        output_ids[0][inputs['input_ids'].shape[-1]:], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    # Cleanup to reduce VRAM usage\n",
    "    del inputs\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a865b915-9cc5-45b0-9f2a-1f95b3a51f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "torch._dynamo.config.cache_size_limit = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7c03ffa-05d1-40ac-9ca6-09f4f7479034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unified_training_data(df1, df2):\n",
    "    \"\"\"Combine both datasets into a unified training format\"\"\"\n",
    "    \n",
    "    print(\"ðŸ”„ Creating unified training dataset...\")\n",
    "    \n",
    "    unified_data = []\n",
    "    \n",
    "    # Process Dataset 1 (Detailed)\n",
    "    print(\"Processing detailed dataset...\")\n",
    "    for _, row in df1.iterrows():\n",
    "        # Basic recommendation\n",
    "        basic_prompt = (\n",
    "            f\"N: {row['Nitrogen']}, P: {row['Phosphorus']}, K: {row['Potassium']}, \"\n",
    "            f\"temperature: {row['Temperature']:.2f}, humidity: {row['Humidity']:.2f}, \"\n",
    "            f\"pH: {row['pH_Value']:.2f}, rainfall: {row['Rainfall']:.2f}\"\n",
    "        )\n",
    "        basic_response = f\"Recommended crop: {row['Crop']}\"\n",
    "        \n",
    "        # Detailed recommendation with soil type and variety\n",
    "        detailed_prompt = (\n",
    "            f\"N: {row['Nitrogen']}, P: {row['Phosphorus']}, K: {row['Potassium']}, \"\n",
    "            f\"temperature: {row['Temperature']:.2f}, humidity: {row['Humidity']:.2f}, \"\n",
    "            f\"pH: {row['pH_Value']:.2f}, rainfall: {row['Rainfall']:.2f}, \"\n",
    "            f\"soil_type: {row['Soil_Type']}\"\n",
    "        )\n",
    "        detailed_response = (\n",
    "            f\"Recommended crop: {row['Crop']}. \"\n",
    "            f\"Suitable soil type: {row['Soil_Type']}. \"\n",
    "            f\"Recommended variety: {row['Variety']}. \"\n",
    "            f\"This combination is optimal for the given soil and climate conditions.\"\n",
    "        )\n",
    "        \n",
    "        # Add both variations\n",
    "        unified_data.append({\"prompt\": basic_prompt, \"response\": basic_response, \"source\": \"dataset1_basic\"})\n",
    "        unified_data.append({\"prompt\": detailed_prompt, \"response\": detailed_response, \"source\": \"dataset1_detailed\"})\n",
    "    \n",
    "    # Process Dataset 2 (Simple)\n",
    "    print(\"Processing simple dataset...\")\n",
    "    for _, row in df2.iterrows():\n",
    "        prompt = (\n",
    "            f\"N: {row['N']}, P: {row['P']}, K: {row['K']}, \"\n",
    "            f\"temperature: {row['temperature']:.2f}, humidity: {row['humidity']:.2f}, \"\n",
    "            f\"pH: {row['ph']:.2f}, rainfall: {row['rainfall']:.2f}\"\n",
    "        )\n",
    "        response = f\"Recommended crop: {row['label']}\"\n",
    "        \n",
    "        unified_data.append({\"prompt\": prompt, \"response\": response, \"source\": \"dataset2\"})\n",
    "    \n",
    "    print(f\"âœ… Created {len(unified_data)} training examples\")\n",
    "    return unified_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a31dc451-84ee-43ed-93ae-2d23240a6d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Creating unified training dataset...\n",
      "Processing detailed dataset...\n",
      "Processing simple dataset...\n",
      "âœ… Created 42200 training examples\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df2 = pd.read_csv('Crop_recommendation.csv')\n",
    "df1  = pd.read_csv('sensor_Crop_Dataset.csv')\n",
    "unified_data = create_unified_training_data(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da221e9b-d5c4-4d1b-a07e-96590a6a0c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nitrogen</th>\n",
       "      <th>Phosphorus</th>\n",
       "      <th>Potassium</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>pH_Value</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Crop</th>\n",
       "      <th>Soil_Type</th>\n",
       "      <th>Variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.074766</td>\n",
       "      <td>53.954402</td>\n",
       "      <td>88.067625</td>\n",
       "      <td>17.261834</td>\n",
       "      <td>72.941652</td>\n",
       "      <td>4.631301</td>\n",
       "      <td>302.842639</td>\n",
       "      <td>Wheat</td>\n",
       "      <td>Clay</td>\n",
       "      <td>Soft Red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107.329352</td>\n",
       "      <td>70.102134</td>\n",
       "      <td>32.081067</td>\n",
       "      <td>21.846116</td>\n",
       "      <td>99.361954</td>\n",
       "      <td>4.761658</td>\n",
       "      <td>94.693847</td>\n",
       "      <td>Tomato</td>\n",
       "      <td>Clay</td>\n",
       "      <td>Beefsteak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>130.634624</td>\n",
       "      <td>67.204533</td>\n",
       "      <td>28.294252</td>\n",
       "      <td>33.246895</td>\n",
       "      <td>81.506836</td>\n",
       "      <td>6.566007</td>\n",
       "      <td>83.563685</td>\n",
       "      <td>Sugarcane</td>\n",
       "      <td>Clay</td>\n",
       "      <td>Co 86032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.169301</td>\n",
       "      <td>87.493181</td>\n",
       "      <td>14.336679</td>\n",
       "      <td>14.396289</td>\n",
       "      <td>59.274465</td>\n",
       "      <td>6.296297</td>\n",
       "      <td>31.508836</td>\n",
       "      <td>Sugarcane</td>\n",
       "      <td>Silt</td>\n",
       "      <td>Co 0238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.881965</td>\n",
       "      <td>89.269712</td>\n",
       "      <td>38.833885</td>\n",
       "      <td>16.773218</td>\n",
       "      <td>51.191584</td>\n",
       "      <td>8.268274</td>\n",
       "      <td>295.193482</td>\n",
       "      <td>Maize</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>Sweet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Nitrogen  Phosphorus  Potassium  Temperature   Humidity  pH_Value  \\\n",
       "0   69.074766   53.954402  88.067625    17.261834  72.941652  4.631301   \n",
       "1  107.329352   70.102134  32.081067    21.846116  99.361954  4.761658   \n",
       "2  130.634624   67.204533  28.294252    33.246895  81.506836  6.566007   \n",
       "3   15.169301   87.493181  14.336679    14.396289  59.274465  6.296297   \n",
       "4   21.881965   89.269712  38.833885    16.773218  51.191584  8.268274   \n",
       "\n",
       "     Rainfall       Crop Soil_Type    Variety  \n",
       "0  302.842639      Wheat      Clay   Soft Red  \n",
       "1   94.693847     Tomato      Clay  Beefsteak  \n",
       "2   83.563685  Sugarcane      Clay   Co 86032  \n",
       "3   31.508836  Sugarcane      Silt    Co 0238  \n",
       "4  295.193482      Maize     Sandy      Sweet  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52feb688-a66a-43c2-a9a8-8e86a491684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unified_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eeafaa9d-e9ba-4da1-bbf0-8393395c4ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ UNIFIED MODEL TRAINING\n",
      "========================================\n",
      "ðŸ”„ Creating unified training dataset...\n",
      "Processing detailed dataset...\n",
      "Processing simple dataset...\n",
      "âœ… Created 42200 training examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb0ab4180cd74c859f1164de25928b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b253c7aebe44777a017960a83b31d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zer0/anaconda3/lib/python3.12/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/home/zer0/anaconda3/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Switching to float32 training since model cannot work with float16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08aa7dd1eef94605957725f5a5216800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=16):   0%|          | 0/42200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 42200 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 42,200 | Num Epochs = 1 | Total steps = 120\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 1,490,944 of 1,001,376,896 (0.15% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 01:45, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.056300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.349800</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.376000</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.004900</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.955500</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.840300</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.919600</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.632500</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.449400</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.246500</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.153400</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.159800</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.964400</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.782000</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.706200</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.732000</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.785100</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.476900</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.498800</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.333300</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.310000</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.266500</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.152100</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.306800</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.152100</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.002000</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.946700</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.046900</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.856600</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.791300</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.860600</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.781700</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.808100</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.763400</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.672000</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.704600</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.601300</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.660200</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.634100</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.550800</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.530700</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.506000</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.521500</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.534700</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.511100</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.556900</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.432300</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.346700</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.520900</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.353800</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.284000</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.511300</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.424400</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.608900</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.531600</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.526700</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.381100</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.616400</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.367000</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.296600</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.318600</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.469300</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.424300</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.422300</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.491800</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.469900</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.397200</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.489400</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.493800</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.479300</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.341300</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.471800</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.559700</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.358200</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.336600</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.408400</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.410700</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.357600</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.332400</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.437900</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>1.253300</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.492800</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>1.394800</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.395300</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.379000</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1.465700</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>1.394700</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.341000</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>1.278000</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.305500</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>1.369900</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.341900</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>1.374500</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>1.386700</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.400400</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.471100</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1.259100</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.435300</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1.343300</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.282500</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>1.366700</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>1.444200</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>1.439500</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>1.349900</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.349500</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>1.302200</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>1.458700</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>1.260600</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>1.378900</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.384800</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>1.413700</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>1.361400</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>1.519500</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>1.319800</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.332500</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>1.539700</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>1.366000</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>1.447300</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>1.290400</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.471800</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=120, training_loss=1.8107136756181716, metrics={'train_runtime': 121.9869, 'train_samples_per_second': 3.935, 'train_steps_per_second': 0.984, 'total_flos': 383510264610816.0, 'train_loss': 1.8107136756181716, 'epoch': 0.011374407582938388})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "    \n",
    "    \n",
    "print(\"ðŸš€ UNIFIED MODEL TRAINING\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Create unified dataset\n",
    "unified_data = create_unified_training_data(df1, df2)\n",
    "\n",
    "# Save as JSONL\n",
    "with open(\"unified_crop_training.jsonl\", \"w\") as f:\n",
    "    for item in unified_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "# Load with datasets\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"unified_crop_training.jsonl\")[\"train\"]\n",
    "\n",
    "# Format for training\n",
    "def formatting_prompts_func(examples):\n",
    "    prompts = examples[\"prompt\"]\n",
    "    responses = examples[\"response\"]\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": p}, {\"role\": \"assistant\", \"content\": r}],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        ).removeprefix(\"<bos>\")\n",
    "        for p, r in zip(prompts, responses)\n",
    "    ]\n",
    "    return {\"text\": texts}\n",
    "\n",
    "formatted_dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "# Train the model\n",
    "#     from peft import LoraConfig, get_peft_model    \n",
    "#     model = FastModel.get_peft_model(\n",
    "#     model,\n",
    "#     finetune_vision_layers     = False, # Turn off for just text!\n",
    "#     finetune_language_layers   = True,  # Should leave on!\n",
    "#     finetune_attention_modules = True,  # Attention good for GRPO\n",
    "#     finetune_mlp_modules       = True,  # Should leave on always!\n",
    "\n",
    "#     r = 8,           # Larger = higher accuracy, but might overfit\n",
    "#     lora_alpha = 8,  # Recommended alpha == r at least\n",
    "#     lora_dropout = 0,\n",
    "#     bias = \"none\",\n",
    "#     random_state = 3407,\n",
    "# )\n",
    "from peft import LoraConfig, get_peft_model\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=formatted_dataset,\n",
    "    args=SFTConfig(\n",
    "        dataset_text_field=\"text\",\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=10,  # Increased for larger dataset\n",
    "        max_steps=120,    # Increased for more data\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=1,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"Training on {len(formatted_dataset)} examples...\")\n",
    "trainer.train()\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13c578bc-2071-4e1a-b13c-37884ced4941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./unified_crop_model/tokenizer_config.json',\n",
       " './unified_crop_model/special_tokens_map.json',\n",
       " './unified_crop_model/chat_template.jinja',\n",
       " './unified_crop_model/tokenizer.model',\n",
       " './unified_crop_model/added_tokens.json',\n",
       " './unified_crop_model/tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.save_pretrained(\"./unified_crop_model\")\n",
    "trainer.processing_class.save_pretrained(\"./unified_crop_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f6168ea-f4d7-432e-89f9-2c95d9a81fe3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SlidingWindowLayer' object has no attribute 'max_batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/unsloth/models/vision.py:236\u001b[0m, in \u001b[0;36munsloth_base_fast_generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode(), autocaster:\n\u001b[0;32m--> 236\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_old_generate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/generation/utils.py:2399\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2398\u001b[0m     max_cache_length \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m inputs_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m-> 2399\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_cache_for_generation(\n\u001b[1;32m   2400\u001b[0m     generation_config, model_kwargs, assistant_model, batch_size, max_cache_length\n\u001b[1;32m   2401\u001b[0m )\n\u001b[1;32m   2403\u001b[0m \u001b[38;5;66;03m# 8. determine generation mode\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/generation/utils.py:1965\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_cache_for_generation\u001b[0;34m(self, generation_config, model_kwargs, assistant_model, batch_size, max_cache_length)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   1962\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `cache_implementation=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mcache_implementation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated. Please only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1963\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSTATIC_CACHE_IMPLEMENTATIONS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, and the layer structure will be inferred automatically.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1964\u001b[0m         )\n\u001b[0;32m-> 1965\u001b[0m     model_kwargs[cache_name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_cache(\n\u001b[1;32m   1966\u001b[0m         cache_implementation\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mcache_implementation,\n\u001b[1;32m   1967\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(generation_config\u001b[38;5;241m.\u001b[39mnum_beams, generation_config\u001b[38;5;241m.\u001b[39mnum_return_sequences) \u001b[38;5;241m*\u001b[39m batch_size,\n\u001b[1;32m   1968\u001b[0m         max_cache_len\u001b[38;5;241m=\u001b[39mmax_cache_length,\n\u001b[1;32m   1969\u001b[0m         model_kwargs\u001b[38;5;241m=\u001b[39mmodel_kwargs,\n\u001b[1;32m   1970\u001b[0m     )\n\u001b[1;32m   1971\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mcache_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/generation/utils.py:1835\u001b[0m, in \u001b[0;36mGenerationMixin._get_cache\u001b[0;34m(self, cache_implementation, batch_size, max_cache_len, model_kwargs)\u001b[0m\n\u001b[1;32m   1830\u001b[0m     cache_to_check \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mself_attention_cache \u001b[38;5;28;01mif\u001b[39;00m requires_cross_attention_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\n\u001b[1;32m   1832\u001b[0m need_new_cache \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1833\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1834\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m cache_to_check\u001b[38;5;241m.\u001b[39moffloading \u001b[38;5;241m!=\u001b[39m offload_cache\n\u001b[0;32m-> 1835\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m cache_to_check\u001b[38;5;241m.\u001b[39mmax_batch_size \u001b[38;5;241m!=\u001b[39m batch_size\n\u001b[1;32m   1836\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m cache_to_check\u001b[38;5;241m.\u001b[39mmax_cache_len \u001b[38;5;241m<\u001b[39m max_cache_len\n\u001b[1;32m   1837\u001b[0m )\n\u001b[1;32m   1839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m requires_cross_attention_cache \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/cache_utils.py:904\u001b[0m, in \u001b[0;36mCache.max_batch_size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the maximum batch size of the cache\"\"\"\u001b[39;00m\n\u001b[0;32m--> 904\u001b[0m values \u001b[38;5;241m=\u001b[39m [layer\u001b[38;5;241m.\u001b[39mmax_batch_size \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers]\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(values)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SlidingWindowLayer' object has no attribute 'max_batch_size'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 23\u001b[0m\n\u001b[1;32m      5\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      6\u001b[0m     {\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     }\n\u001b[1;32m     14\u001b[0m ]\n\u001b[1;32m     16\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(\n\u001b[1;32m     17\u001b[0m     messages,\n\u001b[1;32m     18\u001b[0m     add_generation_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     22\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[1;32m     25\u001b[0m     max_new_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m, \n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Recommended Gemma-3 settings!\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m, top_p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.95\u001b[39m, top_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(outputs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/unsloth/models/vision.py:241\u001b[0m, in \u001b[0;36munsloth_base_fast_generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_lookup_num_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode(), autocaster:\n\u001b[0;32m--> 241\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_old_generate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/generation/utils.py:2399\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2394\u001b[0m     inputs_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m input_ids_length\n\u001b[1;32m   2395\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m model_input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2396\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder\n\u001b[1;32m   2397\u001b[0m ):\n\u001b[1;32m   2398\u001b[0m     max_cache_length \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m inputs_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m-> 2399\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_cache_for_generation(\n\u001b[1;32m   2400\u001b[0m     generation_config, model_kwargs, assistant_model, batch_size, max_cache_length\n\u001b[1;32m   2401\u001b[0m )\n\u001b[1;32m   2403\u001b[0m \u001b[38;5;66;03m# 8. determine generation mode\u001b[39;00m\n\u001b[1;32m   2404\u001b[0m generation_mode \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mget_generation_mode(assistant_model)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/generation/utils.py:1965\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_cache_for_generation\u001b[0;34m(self, generation_config, model_kwargs, assistant_model, batch_size, max_cache_length)\u001b[0m\n\u001b[1;32m   1960\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mcache_implementation \u001b[38;5;129;01min\u001b[39;00m DEPRECATED_STATIC_CACHE_IMPLEMENTATIONS:\n\u001b[1;32m   1961\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   1962\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `cache_implementation=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mcache_implementation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated. Please only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1963\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSTATIC_CACHE_IMPLEMENTATIONS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, and the layer structure will be inferred automatically.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1964\u001b[0m         )\n\u001b[0;32m-> 1965\u001b[0m     model_kwargs[cache_name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_cache(\n\u001b[1;32m   1966\u001b[0m         cache_implementation\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mcache_implementation,\n\u001b[1;32m   1967\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(generation_config\u001b[38;5;241m.\u001b[39mnum_beams, generation_config\u001b[38;5;241m.\u001b[39mnum_return_sequences) \u001b[38;5;241m*\u001b[39m batch_size,\n\u001b[1;32m   1968\u001b[0m         max_cache_len\u001b[38;5;241m=\u001b[39mmax_cache_length,\n\u001b[1;32m   1969\u001b[0m         model_kwargs\u001b[38;5;241m=\u001b[39mmodel_kwargs,\n\u001b[1;32m   1970\u001b[0m     )\n\u001b[1;32m   1971\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mcache_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1972\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_supports_default_dynamic_cache():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/generation/utils.py:1835\u001b[0m, in \u001b[0;36mGenerationMixin._get_cache\u001b[0;34m(self, cache_implementation, batch_size, max_cache_len, model_kwargs)\u001b[0m\n\u001b[1;32m   1829\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1830\u001b[0m     cache_to_check \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mself_attention_cache \u001b[38;5;28;01mif\u001b[39;00m requires_cross_attention_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\n\u001b[1;32m   1832\u001b[0m need_new_cache \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1833\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1834\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m cache_to_check\u001b[38;5;241m.\u001b[39moffloading \u001b[38;5;241m!=\u001b[39m offload_cache\n\u001b[0;32m-> 1835\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m cache_to_check\u001b[38;5;241m.\u001b[39mmax_batch_size \u001b[38;5;241m!=\u001b[39m batch_size\n\u001b[1;32m   1836\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m cache_to_check\u001b[38;5;241m.\u001b[39mmax_cache_len \u001b[38;5;241m<\u001b[39m max_cache_len\n\u001b[1;32m   1837\u001b[0m )\n\u001b[1;32m   1839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m requires_cross_attention_cache \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1840\u001b[0m     need_new_cache \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1841\u001b[0m         need_new_cache\n\u001b[1;32m   1842\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mcross_attention_cache\u001b[38;5;241m.\u001b[39mmax_cache_len \u001b[38;5;241m!=\u001b[39m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1843\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/cache_utils.py:904\u001b[0m, in \u001b[0;36mCache.max_batch_size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmax_batch_size\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    903\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the maximum batch size of the cache\"\"\"\u001b[39;00m\n\u001b[0;32m--> 904\u001b[0m     values \u001b[38;5;241m=\u001b[39m [layer\u001b[38;5;241m.\u001b[39mmax_batch_size \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers]\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(values)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    906\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax batch size is not consistent across layers: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SlidingWindowLayer' object has no attribute 'max_batch_size'"
     ]
    }
   ],
   "source": [
    " \n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant that recommends crops based on soil and climate.\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"N: 80, P: 38, K: 67, temperature: 32.01, humidity: 58, pH: 6.90, rainfall: 302.93\"}]\n",
    "    }\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, \n",
    "    return_tensors = \"pt\",\n",
    "    tokenize = True,\n",
    "    return_dict = True,\n",
    ").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens = 128, \n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    ")\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029b5906-fd60-4eb5-9a4f-6e08714b832e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
